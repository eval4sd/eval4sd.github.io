---
permalink: /cfp/
title: "Eval4SD - Call for Papers"
layout: single
author_profile: false
header:
  overlay_image: /low-poly-grid-haikei.svg
  overlay_filter: 0.5
  overlay_color: "#2b5f8e"
---

We invite submissions on the evaluation of large language models in specialized 
domains such as but not limited to law, medicine, science, finance, education, and policy.
All submissions must follow the [ACL formatting guidelines](https://github.com/acl-org/acl-style-files) 
and be submitted through OpenReview. Submissions must be anonymized for double-blind review.

## Submission Types

**Long Papers (up to 8 pages + references)**  
Complete research contributions with novel findings, experimental results, and 
thorough analysis. Suitable for mature work on LLM evaluation methodology or 
new benchmark proposals.

**Short & Position Papers (up to 4 pages + references)**  
Preliminary results, position papers, system descriptions, and focused 
contributions. Great for provocative arguments or narrowly scoped empirical studies.


## Topics of Interest

We invite submissions on (but not limited to) the following topic areas:

- **LLM Benchmarking:** We invite contributions that evaluate multiple models, datasets, inference methods, or prompting techniques on existing data or introduce novel, specialized benchmarking datasets. Papers in this direction may seek to answer questions like: 'Which model should I use for my social science project?'  'Are open-weight models inferior for specialized tasks?', or 'Given a limited budget, what is my best choice of LLM for my digital humanities question?' We especially encourage submissions that evaluate performance in low- and medium-resource languages. 
- **Domain Research Replication:** Does information automatically extracted using a different model or a slightly altered approach still support the same domain conclusions? We invite submissions that attempt to replicate existing domain research using a tweaked LLM setup. For us, testing open-weight models is especially important in light of replicability. We are excited to see how robust domain research is to adaptations of the automation setups, from prompting to model weights and training data.
- **Metrics and Evaluation Methodology:** We invite submissions on methodology for assessing LLM outputs in complex tasks. This includes work on LLM judge setups or novel rule-based metrics for specialized tasks.

## Important Dates

| | |
|---|---|
| Submissions open | TBD |
| Submission deadline | TBD |
| Notification of acceptance | TBD |
| Camera-ready deadline | August 15, 2026 |
| Workshop date | September 2026 |

*All deadlines are 11:59 PM CEST.*

## Submission Link

Submissions via **[OpenReview](#)** *(link to be added)*.

All submissions undergo double-blind peer review. Submitted papers must not be under review elsewhere.
